{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAT utils\n",
    "\n",
    "> Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "from fastai.test_utils import synth_learner\n",
    "from fastai.callback.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recorder customization\n",
    "\n",
    "Patches `Recorder` to enable adding train-only metrics. Can be used to log adversarial loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def before_fit(self:Recorder):\n",
    "        \"Prepare state for training\"\n",
    "        self.lrs,self.iters,self.losses,self.values = [],[],[],[]\n",
    "        names = self.metrics.attrgot('name')\n",
    "        train_only_names = self.train_only_metrics.attrgot('name')\n",
    "        if self.train_metrics and self.valid_metrics:\n",
    "            names = L('loss') + names\n",
    "            names = names.map('train_{}') + train_only_names + names.map('valid_{}')\n",
    "        elif self.valid_metrics: names = L('train_loss', *train_only_names, 'valid_loss') + names\n",
    "        else: names = L('train_loss') + train_only_names + names\n",
    "        if self.add_time: names.append('time')\n",
    "        self.metric_names = 'epoch'+names\n",
    "        self.smooth_loss.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch(as_prop=True)\n",
    "def _train_mets(self:Recorder):\n",
    "    if getattr(self, 'cancel_train', False): return L()\n",
    "    return L(self.smooth_loss) + self.train_only_metrics + (self.metrics if self.train_metrics else L())\n",
    "\n",
    "@patch(as_prop=True)\n",
    "def train_only_metrics(self:Recorder):\n",
    "    return getattr(self, '_train_only_metrics', L())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def add_train_metrics(self:Recorder, *metrics):\n",
    "    self._train_only_metrics = self.train_only_metrics\n",
    "    self._train_only_metrics += L(*metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>nothing</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>15.411602</td>\n",
       "      <td>0</td>\n",
       "      <td>11.542326</td>\n",
       "      <td>3.397400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.509046</td>\n",
       "      <td>0</td>\n",
       "      <td>8.202074</td>\n",
       "      <td>2.863926</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _met_func():\n",
    "    return 0\n",
    "metric = ValueMetric(_met_func, 'nothing')\n",
    "learn = synth_learner(metrics=rmse)\n",
    "learn.recorder.add_train_metrics(metric)\n",
    "assert learn.recorder._train_only_metrics[0] is metric\n",
    "learn.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandbCallback customization\n",
    "\n",
    "1. Record the best values per metric as well as current one - used for post-analisys\n",
    "2. Log additional values found `learn.log_extras` - used to log losses computed by callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "try:\n",
    "    import wandb\n",
    "    from fastai.callback.wandb import *\n",
    "    from fastai.callback.wandb import _make_plt, _format_config_value, _format_config, _format_metadata\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "class WandbCallback(Callback):\n",
    "    \"Saves model topology, losses & metrics\"\n",
    "    remove_on_fetch,order = True,Recorder.order+1\n",
    "    # Record if watch has been called previously (even in another instance)\n",
    "    _wandb_watch_called = False\n",
    "\n",
    "    def __init__(self, log=\"gradients\", log_preds=True, log_model=True, log_dataset=False, dataset_name=None, valid_dl=None, n_preds=36, seed=12345, reorder=True, compare=None):\n",
    "        # Check if wandb.init has been called\n",
    "        if wandb.run is None:\n",
    "            raise ValueError('You must call wandb.init() before WandbCallback()')\n",
    "        # W&B log step\n",
    "        self._wandb_step = wandb.run.step - 1  # -1 except if the run has previously logged data (incremented at each batch)\n",
    "        self._wandb_epoch = 0 if not(wandb.run.step) else math.ceil(wandb.run.summary['epoch']) # continue to next epoch\n",
    "        store_attr('log,log_preds,log_model,log_dataset,dataset_name,valid_dl,n_preds,seed,reorder,compare')\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Call watch method to log model topology, gradients & weights\"\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\") and rank_distrib()==0\n",
    "        if not self.run: return\n",
    "\n",
    "        # Log config parameters\n",
    "        log_config = self.learn.gather_args()\n",
    "        _format_config(log_config)\n",
    "        try:\n",
    "            wandb.config.update(log_config, allow_val_change=True)\n",
    "        except Exception as e:\n",
    "            print(f'WandbCallback could not log config parameters -> {e}')\n",
    "\n",
    "        if not WandbCallback._wandb_watch_called:\n",
    "            WandbCallback._wandb_watch_called = True\n",
    "            # Logs model topology and optionally gradients and weights\n",
    "            wandb.watch(self.learn.model, log=self.log)\n",
    "\n",
    "        # log dataset\n",
    "        assert isinstance(self.log_dataset, (str, Path, bool)), 'log_dataset must be a path or a boolean'\n",
    "        if self.log_dataset is True:\n",
    "            if Path(self.dls.path) == Path('.'):\n",
    "                print('WandbCallback could not retrieve the dataset path, please provide it explicitly to \"log_dataset\"')\n",
    "                self.log_dataset = False\n",
    "            else:\n",
    "                self.log_dataset = self.dls.path\n",
    "        if self.log_dataset:\n",
    "            self.log_dataset = Path(self.log_dataset)\n",
    "            assert self.log_dataset.is_dir(), f'log_dataset must be a valid directory: {self.log_dataset}'\n",
    "            metadata = {'path relative to learner': os.path.relpath(self.log_dataset, self.learn.path)}\n",
    "            log_dataset(path=self.log_dataset, name=self.dataset_name, metadata=metadata)\n",
    "\n",
    "        # log model\n",
    "        if self.log_model and not hasattr(self, 'save_model'):\n",
    "            print('WandbCallback requires use of \"SaveModelCallback\" to log best model')\n",
    "            self.log_model = False\n",
    "\n",
    "        if self.log_preds:\n",
    "            try:\n",
    "                if not self.valid_dl:\n",
    "                    #Initializes the batch watched\n",
    "                    wandbRandom = random.Random(self.seed)  # For repeatability\n",
    "                    self.n_preds = min(self.n_preds, len(self.dls.valid_ds))\n",
    "                    idxs = wandbRandom.sample(range(len(self.dls.valid_ds)), self.n_preds)\n",
    "                    if isinstance(self.dls,  TabularDataLoaders):\n",
    "                        test_items = getattr(self.dls.valid_ds.items, 'iloc', self.dls.valid_ds.items)[idxs]\n",
    "                        self.valid_dl = self.dls.test_dl(test_items, with_labels=True, process=False)\n",
    "                    else:\n",
    "                        test_items = [getattr(self.dls.valid_ds.items, 'iloc', self.dls.valid_ds.items)[i] for i in idxs]\n",
    "                        self.valid_dl = self.dls.test_dl(test_items, with_labels=True)\n",
    "                self.learn.add_cb(FetchPredsCallback(dl=self.valid_dl, with_input=True, with_decoded=True, reorder=self.reorder))\n",
    "            except Exception as e:\n",
    "                self.log_preds = False\n",
    "                print(f'WandbCallback was not able to prepare a DataLoader for logging prediction samples -> {e}')\n",
    "\n",
    "        self.best_metrics = {}\n",
    "        if self.compare is None:\n",
    "            self.compare = [np.less if (('loss' in n) or ('error' in n)) else np.greater for n in self.recorder.metric_names[2:-1]]\n",
    "        elif not islisty(self.compare): self.compare = [self.compare]\n",
    "\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Log hyper-parameters and training loss\"\n",
    "        if self.training:\n",
    "            self._wandb_step += 1\n",
    "            self._wandb_epoch += 1/self.n_iter\n",
    "            hypers = {f'{k}_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "            log_extras = getattr(self.learn, 'log_extras', {})\n",
    "            wandb.log({'epoch': self._wandb_epoch, 'train_loss': to_detach(self.smooth_loss.clone()), 'raw_loss': to_detach(self.loss.clone()), **hypers, **log_extras}, step=self._wandb_step)\n",
    "\n",
    "    def log_predictions(self, preds):\n",
    "        inp,preds,targs,out = preds\n",
    "        b = tuplify(inp) + tuplify(targs)\n",
    "        x,y,its,outs = self.valid_dl.show_results(b, out, show=False, max_n=self.n_preds)\n",
    "        wandb.log(wandb_process(x, y, its, outs), step=self._wandb_step)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Log validation loss and custom metrics & log prediction samples\"\n",
    "        # Correct any epoch rounding error and overwrite value\n",
    "        self._wandb_epoch = round(self._wandb_epoch)\n",
    "        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n",
    "        # Log sample predictions\n",
    "        if self.log_preds:\n",
    "            try:\n",
    "                self.log_predictions(self.learn.fetch_preds.preds)\n",
    "            except Exception as e:\n",
    "                self.log_preds = False\n",
    "                print(f'WandbCallback was not able to get prediction samples -> {e}')\n",
    "        metrics = {n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}\n",
    "        wandb.log(metrics, step=self._wandb_step)\n",
    "        self.update_best(metrics)\n",
    "        wandb.log(self.best_metrics, step=self._wandb_step)\n",
    "\n",
    "\n",
    "    def after_fit(self):\n",
    "        if self.log_model:\n",
    "            if self.save_model.last_saved_path is None:\n",
    "                print('WandbCallback could not retrieve a model to upload')\n",
    "            else:\n",
    "                metadata = {n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}\n",
    "                log_model(self.save_model.last_saved_path, metadata=metadata)\n",
    "        self.run = True\n",
    "        if self.log_preds: self.remove_cb(FetchPredsCallback)\n",
    "        wandb.log({})  # ensure sync of last step\n",
    "        self._wandb_step += 1\n",
    "\n",
    "    def update_best(self, metrics):\n",
    "        for is_better, (n,v) in zip(self.compare, metrics.items()):\n",
    "            current_best = self.best_metrics.get(f'best_{n}', None)\n",
    "            if current_best is None or is_better(v, current_best):\n",
    "                self.best_metrics[f'best_{n}'] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('torchenv': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd04af15c6377fd3f0f03723b0d6472f0c10dcd7b2afd49a75a2b51cefc0e0a1d19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
