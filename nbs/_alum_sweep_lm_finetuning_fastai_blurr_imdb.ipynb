{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 17 10:19:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:65:00.0 Off |                  N/A |\n",
      "| 31%   25C    P8    12W / 250W |     21MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1348      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1424      G   /usr/bin/gnome-shell                6MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -Uqq fastcore onnx onnxruntime sentencepiece seqeval rouge-score\n",
    "    !pip install -Uqq --no-deps fastai ohmeow-blurr\n",
    "    !pip install -Uqq transformers datasets wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import wandb\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(fn):\n",
    "    return open(fn).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "max_len = 512\n",
    "bs = 8\n",
    "val_bs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_device(e, device):\n",
    "    if hasattr(e, 'to'): return e.to(device)\n",
    "    elif isinstance(e, dict):\n",
    "        for _, v in e.items():\n",
    "            if hasattr(v, 'to'): v.to(device)\n",
    "        return {k:(v.to(device) if hasattr(v, 'to') else v) for k, v in e.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def one_batch(self:Learner, i, b):\n",
    "        self.iter = i\n",
    "        b_on_device = tuple(_to_device(e, self.dls.device) for e in b) if self.dls.device is not None else b\n",
    "        self._split(b_on_device)\n",
    "        self._with_events(self._do_one_batch, 'batch', CancelBatchException)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/morgan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, model_cls=AutoModelForSequenceClassification,\n",
    "                                                                               tokenizer_cls=AutoTokenizer, tokenizer_kwargs={'max_len':512})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_items=get_text_files,\n",
    "                   get_x = read_text,\n",
    "                   get_y=parent_label,\n",
    "                   splitter=GrandparentSplitter(valid_name='test'))\n",
    "\n",
    "dls = dblock.dataloaders(path, bs=bs, val_bs=val_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vat finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import linalg as LA\n",
    "\n",
    "def KL(input, target, reduction=\"sum\"):\n",
    "    input = input.float()\n",
    "    target = target.float()\n",
    "    loss = F.kl_div(F.log_softmax(input, dim=-1, dtype=torch.float32), F.softmax(target, dim=-1, dtype=torch.float32), reduction=reduction)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.all import Hook\n",
    "\n",
    "def hook_out(m, inp, out):\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_project(grad, norm_type='inf', eps=1e-6):\n",
    "    if norm_type == 'l2':\n",
    "        direction = grad / (torch.norm(grad, dim=-1, keepdim=True) + eps)\n",
    "    elif norm_type == 'l1':\n",
    "        direction = grad.sign()\n",
    "    else:\n",
    "        direction = grad / (grad.abs().max(-1, keepdim=True)[0] + eps)\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adversarial_loss(model:nn.Module, embed:Tensor, logits:Tensor, \n",
    "                             noise_var:float=1e-5, step_size:float=1e-3, k:int=1,\n",
    "                             noise_gamma:float=1e-6):\n",
    "    \"This is nice docstring\"\n",
    "    noise = embed.data.new(embed.size()).normal_(0, noise_var) \n",
    "    noise.requires_grad_();\n",
    "\n",
    "    for _ in range(k):\n",
    "        newembed = embed + noise\n",
    "        adv_logits = model(inputs_embeds=newembed).logits\n",
    "\n",
    "        adv_loss = KL(adv_logits, logits.detach(), reduction=\"batchmean\") \n",
    "        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True)\n",
    "\n",
    "        norm = LA.norm(delta_grad) \n",
    "        if (torch.isnan(norm) or torch.isinf(norm)):\n",
    "            break\n",
    "\n",
    "        noise = noise + delta_grad * step_size\n",
    "        noise = adv_project(noise, norm_type=\"fro\", eps=noise_gamma)\n",
    "\n",
    "    newembed = embed + noise\n",
    "    adv_logits = model(inputs_embeds=newembed).logits\n",
    "\n",
    "    adv_loss_f = KL(adv_logits, logits.detach())\n",
    "    adv_loss_b = KL(logits, adv_logits.detach())\n",
    "    return adv_loss_f + adv_loss_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALUMCallback(Callback):\n",
    "    \"ALUM callback (draft)\"\n",
    "    run_valid = False\n",
    "    order = GradientAccumulation.order-1\n",
    "    @delegates(compute_adversarial_loss)\n",
    "    def __init__(self, m:nn.Module, alpha:float=1., start_epoch:int=1, **kwargs):\n",
    "        self.hook = None\n",
    "        self.adv_loss_func = partial(compute_adversarial_loss, **kwargs) if kwargs else compute_adversarial_loss\n",
    "        store_attr()\n",
    "\n",
    "    def before_batch(self):\n",
    "        if (self.hook is None) and (self.epoch >= self.start_epoch):\n",
    "            self.hook = Hook(self.m, hook_out)\n",
    "            print(f'Starting virtual adversarial training at epoch {self.epoch}')\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self.epoch >= self.start_epoch:\n",
    "            embed, logits = self.hook.stored, self.pred\n",
    "            adv_loss = self.adv_loss_func(self.model.hf_model, embed, logits)\n",
    "            self.learn.loss_grad += adv_loss * self.alpha\n",
    "\n",
    "    def after_fit(self):\n",
    "        if self.hook is not None: self.hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a HyperParameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmorgan\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    run = wandb.init();\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(\n",
    "        model_name, model_cls=AutoModelForSequenceClassification, \n",
    "        tokenizer_cls=AutoTokenizer, tokenizer_kwargs={'max_len':512})\n",
    "    \n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls,\n",
    "                    model,\n",
    "                    opt_func=RAdam,\n",
    "                    metrics=[accuracy],\n",
    "                    cbs=[HF_BaseModelCallback, GradientAccumulation(8)],\n",
    "                    splitter=hf_splitter).to_fp16()\n",
    "\n",
    "    learn.add_cb(ALUMCallback(learn.model.hf_model.base_model.embeddings, \n",
    "                             start_epoch = run.config.start_epoch,\n",
    "                             alpha=run.config.alpha,\n",
    "                             noise_var=run.config.noise_var,\n",
    "                             noise_gamma =run.config.noise_gamma,\n",
    "                             step_size=run.config.step_size\n",
    "                             ));\n",
    "\n",
    "    learn.fit_one_cycle(4, 2e-5, cbs=[WandbCallback(log_preds=False, log_model=False)])\n",
    "    del learn\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"name\": \"ALUM test sweep\",\n",
    "  \"method\": \"random\",\n",
    "  \"parameters\": {\n",
    "        \"start_epoch\": {\"values\":[0,1]},\n",
    "        \"alpha\": {\"values\": [0.0, 0.25,0.5,1,2,4,8,10,20]},\n",
    "        \"noise_var\": {\"values\": [1e-6, 1e-5, 1e-4, 1e-3]},\n",
    "        \"noise_gamma\": {\"values\": [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]},\n",
    "        \"step_size\": {\"values\": [1e-5, 1e-4, 1e-3, 1e-2]},   \n",
    "    },\n",
    "  \"metric\":{\"goal\": \"maximise\", \"name\": \"accuracy\"},\n",
    "  \"early_terminate\": {\"type\": \"hyperband\", \"s\": 2, \"eta\": 3, \"max_iter\": 60}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 5x4o4q3e\n",
      "Sweep URL: https://wandb.ai/fastai_community/vat/sweeps/5x4o4q3e\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"vat\", entity=\"fastai_community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mrhnqn9y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_gamma: 1e-07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_var: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstart_epoch: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstep_size: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dashing-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/fastai_community/vat\" target=\"_blank\">https://wandb.ai/fastai_community/vat</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/fastai_community/vat/sweeps/5x4o4q3e\" target=\"_blank\">https://wandb.ai/fastai_community/vat/sweeps/5x4o4q3e</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/fastai_community/vat/runs/mrhnqn9y\" target=\"_blank\">https://wandb.ai/fastai_community/vat/runs/mrhnqn9y</a><br/>\n",
       "                Run data is saved locally in <code>/home/morgan/ml/projects/vat/nbs/wandb/run-20210417_103041-mrhnqn9y</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not gather input dimensions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/4 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='557' class='' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      17.82% [557/3125 01:34<07:17 0.6040]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual adversarial training at epoch 0\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
