{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  4 20:16:41 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -Uqq fastcore onnx onnxruntime sentencepiece seqeval rouge-score\n",
    "    !pip install -Uqq --no-deps fastai ohmeow-blurr\n",
    "    !pip install -Uqq transformers datasets wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from fastai.callback.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(fn):\n",
    "    return open(fn).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "max_len = 512\n",
    "bs = 8\n",
    "val_bs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "WANDB_NAME = f'imdb-{model_name}-simple'\n",
    "GROUP = f'IMDB-{model_name}-simple'\n",
    "NOTES = f'Simple finetuning {model_name} with RAdam lr=2e-5'\n",
    "CONFIG = {}\n",
    "TAGS =[model_name,'imdb','radam','simple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfastai_community\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.24<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">imdb-distilbert-base-uncased-simple</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/fastai_community/vat\" target=\"_blank\">https://wandb.ai/fastai_community/vat</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/fastai_community/vat/runs/db5ahwp1\" target=\"_blank\">https://wandb.ai/fastai_community/vat/runs/db5ahwp1</a><br/>\n",
       "                Run data is saved locally in <code>/content/wandb/run-20210404_201708-db5ahwp1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(reinit=True, project=\"vat\", entity=\"fastai_community\",\n",
    "           name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS, config=CONFIG);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_device(e, device):\n",
    "    if hasattr(e, 'to'): return e.to(device)\n",
    "    elif isinstance(e, dict):\n",
    "        for _, v in e.items():\n",
    "            if hasattr(v, 'to'): v.to(device)\n",
    "        return {k:(v.to(device) if hasattr(v, 'to') else v) for k, v in e.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def one_batch(self:Learner, i, b):\n",
    "        self.iter = i\n",
    "        b_on_device = tuple(_to_device(e, self.dls.device) for e in b) if self.dls.device is not None else b\n",
    "        self._split(b_on_device)\n",
    "        self._with_events(self._do_one_batch, 'batch', CancelBatchException)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, task=task, tokenizer_kwargs={'max_len':512})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_items=get_text_files,\n",
    "                   get_x = read_text,\n",
    "                   get_y=parent_label,\n",
    "                   splitter=GrandparentSplitter(valid_name='test'))\n",
    "\n",
    "dls = dblock.dataloaders(path, bs=bs, val_bs=val_bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "HF_BaseModelWrapper (Input shape: 8 x 512)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     8 x 512 x 768       \n",
       "Embedding                                 23440896   True      \n",
       "Embedding                                 393216     True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "____________________________________________________________________________\n",
       "                     8 x 512 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "Linear                                    590592     True      \n",
       "____________________________________________________________________________\n",
       "                     8 x 2               \n",
       "Linear                                    1538       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 66,955,010\n",
       "Total trainable params: 66,955,010\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: <function RAdam at 0x7f8f96fc40e0>\n",
       "Loss function: FlattenedLoss of CrossEntropyLoss()\n",
       "\n",
       "Model unfrozen\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback\n",
       "  - HF_BaseModelCallback\n",
       "  - MixedPrecision"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "learn = Learner(dls,\n",
    "                model,\n",
    "                opt_func=RAdam,\n",
    "                metrics=[accuracy],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter).to_fp16()\n",
    "\n",
    "learn.create_opt() \n",
    "learn.unfreeze()\n",
    "learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fit\n",
      "   - before_fit     : [TrainEvalCallback, MixedPrecision, Recorder, ProgressCallback]\n",
      "  Start Epoch Loop\n",
      "     - before_epoch   : [Recorder, ProgressCallback]\n",
      "    Start Train\n",
      "       - before_train   : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - before_batch   : [HF_BaseModelCallback, MixedPrecision]\n",
      "         - after_pred     : [HF_BaseModelCallback, MixedPrecision]\n",
      "         - after_loss     : [HF_BaseModelCallback, MixedPrecision]\n",
      "         - before_backward: [MixedPrecision]\n",
      "         - before_step    : [MixedPrecision]\n",
      "         - after_step     : [MixedPrecision]\n",
      "         - after_cancel_batch: []\n",
      "         - after_batch    : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      End Batch Loop\n",
      "    End Train\n",
      "     - after_cancel_train: [Recorder]\n",
      "     - after_train    : [Recorder, ProgressCallback]\n",
      "    Start Valid\n",
      "       - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - **CBs same as train batch**: []\n",
      "      End Batch Loop\n",
      "    End Valid\n",
      "     - after_cancel_validate: [Recorder]\n",
      "     - after_validate : [Recorder, ProgressCallback]\n",
      "  End Epoch Loop\n",
      "   - after_cancel_epoch: []\n",
      "   - after_epoch    : [Recorder]\n",
      "End Fit\n",
      " - after_cancel_fit: []\n",
      " - after_fit      : [ProgressCallback]\n"
     ]
    }
   ],
   "source": [
    "learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not gather input dimensions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.243032</td>\n",
       "      <td>0.246754</td>\n",
       "      <td>0.890720</td>\n",
       "      <td>11:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.146905</td>\n",
       "      <td>0.185891</td>\n",
       "      <td>0.926920</td>\n",
       "      <td>11:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077715</td>\n",
       "      <td>0.223364</td>\n",
       "      <td>0.934840</td>\n",
       "      <td>11:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034727</td>\n",
       "      <td>0.265831</td>\n",
       "      <td>0.934840</td>\n",
       "      <td>11:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "Exception ignored in: <finalize object at 0x7f8f04744700; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/weakref.py\", line 572, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"/usr/lib/python3.7/tempfile.py\", line 936, in _cleanup\n",
      "    _rmtree(name)\n",
      "  File \"/usr/lib/python3.7/shutil.py\", line 485, in rmtree\n",
      "    onerror(os.lstat, path, sys.exc_info())\n",
      "  File \"/usr/lib/python3.7/shutil.py\", line 483, in rmtree\n",
      "    orig_st = os.lstat(path)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpqxxgbj5l'\n",
      "Exception ignored in: <finalize object at 0x7f8f04744700; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/weakref.py\", line 572, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"/usr/lib/python3.7/tempfile.py\", line 936, in _cleanup\n",
      "    _rmtree(name)\n",
      "  File \"/usr/lib/python3.7/shutil.py\", line 485, in rmtree\n",
      "    onerror(os.lstat, path, sys.exc_info())\n",
      "  File \"/usr/lib/python3.7/shutil.py\", line 483, in rmtree\n",
      "    orig_st = os.lstat(path)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpqxxgbj5l'\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, 2e-5, cbs=WandbCallback(log_preds=False, log_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
