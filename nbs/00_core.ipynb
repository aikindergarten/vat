{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAT\n",
    "\n",
    "> wip..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "from fastai.test_utils import synth_learner\n",
    "from fastai.callback.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LossCallback(Callback):\n",
    "    \"Base class for loss-computing callbacks\"\n",
    "    def log_loss(self, loss:torch.Tensor, log_name:str):\n",
    "        \"Write `loss` item to `self.learn.log_extras`\"\n",
    "        log_extras = getattr(self.learn, 'log_extras', {})\n",
    "        log_extras[log_name] = loss.detach().item()\n",
    "        self.learn.log_extras = log_extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALUM\n",
    "\n",
    "Adversarial training for large neural language models as presented in https://arxiv.org/abs/2004.08994."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hook_out(m, inp, out):\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def KL(inp, targ, reduction=\"sum\"):\n",
    "    inp = inp.float()\n",
    "    targ = targ.float()\n",
    "    return F.kl_div(F.log_softmax(inp, dim=-1, dtype=torch.float32), F.softmax(targ, dim=-1, dtype=torch.float32), reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SymmetrizedKL(inp, targ, reduction=\"sum\"):\n",
    "    return KL(inp, targ.detach(), reduction=reduction) + KL(targ, inp.detach(), reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def adv_project(grad, norm_type='inf', eps=1e-6):\n",
    "    if norm_type == 'l2':\n",
    "        direction = grad / (torch.norm(grad, dim=-1, keepdim=True) + eps)\n",
    "    elif norm_type == 'l1':\n",
    "        direction = grad.sign()\n",
    "    else:\n",
    "        direction = grad / (grad.abs().max(-1, keepdim=True)[0] + eps)\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_adversarial_loss(model:nn.Module, embed:Tensor, logits:Tensor,\n",
    "                             special_tokens_mask=None, token_type_mask=None,\n",
    "                             noise_var:float=1e-5, step_size:float=1e-3, k:int=1,\n",
    "                             noise_gamma:float=1e-6, criterion=SymmetrizedKL):\n",
    "    \"Computes adversarial loss on iteratively refined perturbation\"\n",
    "    noise = embed.data.new(embed.size()).normal_(0, noise_var)\n",
    "    noise.requires_grad_();\n",
    "    if special_tokens_mask is not None:\n",
    "        noise = noise*special_tokens_mask\n",
    "    if token_type_mask is not None:\n",
    "        nosie = noise*token_type_mask\n",
    "\n",
    "    for _ in range(k):\n",
    "        newembed = embed + noise\n",
    "        adv_logits = model(inputs_embeds=newembed).logits\n",
    "\n",
    "        adv_loss = KL(adv_logits, logits.detach(), reduction=\"batchmean\")\n",
    "        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True)\n",
    "\n",
    "        norm = torch.linalg.norm(delta_grad)\n",
    "        if (torch.isnan(norm) or torch.isinf(norm)):\n",
    "            break\n",
    "        noise = noise + delta_grad * step_size\n",
    "        noise = adv_project(noise, norm_type=\"fro\", eps=noise_gamma)\n",
    "\n",
    "    newembed = embed + noise\n",
    "    adv_logits = model(inputs_embeds=newembed).logits\n",
    "\n",
    "    return criterion(adv_logits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class ALUMCallback(LossCallback):\n",
    "    \"ALUM callback for HuggingFace pretrained models\"\n",
    "    run_valid = False\n",
    "    order = GradientAccumulation.order-1\n",
    "    @delegates(compute_adversarial_loss)\n",
    "    def __init__(self, m:nn.Module, alpha:float=1., start_epoch:int=0,\n",
    "                 criterion=None, mask_special_tokens:bool=False, \n",
    "                 one_token_type=False, **kwargs):\n",
    "        self.hook = None\n",
    "        self.kwargs = kwargs if kwargs else {}\n",
    "        self._do_vat=True\n",
    "        self.special_tokens_mask, self.token_type_mask = None, None\n",
    "        store_attr()\n",
    "    \n",
    "    def before_fit(self):\n",
    "        if self.criterion is None:\n",
    "            self.criterion = MSELoss() if isinstance(self.loss_func, nn.MSELoss) else SymmetrizedKL\n",
    "        self.adv_loss_func = partial(compute_adversarial_loss, criterion=self.criterion, **self.kwargs)\n",
    "    \n",
    "    def before_batch(self):\n",
    "        if (self.hook is None) and (self.epoch >= self.start_epoch):\n",
    "            self.hook = Hook(self.m, hook_out)\n",
    "            print(f'Starting virtual adversarial training at epoch {self.epoch}')\n",
    "\n",
    "        if self.mask_special_tokens:\n",
    "            self.special_tokens_mask = self.xb[0].pop('special_tokens_mask', None)\n",
    "            if self.special_tokens_mask is not None:\n",
    "                self.special_tokens_mask = (1-self.special_tokens_mask).unsqueeze(-1)\n",
    "        if self.one_token_type:\n",
    "            self.token_type_mask = self.xb[0].pop('token_type_ids', None)\n",
    "            if self.token_type_mask is not None:\n",
    "                # this would deterministically mask tokens of type 0\n",
    "                self.token_type_mask = self.token_type_mask.unsqueeze(-1)\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self.epoch >= self.start_epoch and self._do_vat:\n",
    "            embed, logits = self.hook.stored, self.pred\n",
    "            model = self.model.hf_model if hasattr(self.model, 'hf_model') else self.model\n",
    "            try:\n",
    "                adv_loss = self.adv_loss_func(model, embed, logits, self.special_tokens_mask, self.token_type_mask)\n",
    "                self.log_loss(adv_loss, 'adversarial_loss')\n",
    "            except TypeError as e:\n",
    "                print(\"Your model is probably not supported, make sure model interface is compatible with HF pretrained models\")\n",
    "                adv_loss, self._do_vat = 0, False\n",
    "            self.learn.loss_grad += adv_loss * self.alpha\n",
    "\n",
    "    def after_fit(self):\n",
    "        if self.hook is not None: self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1,10, bias=False),\n",
    "    nn.Linear(10,1, bias=False)\n",
    ")\n",
    "learn = synth_learner(model=model, cbs=ALUMCallback(model[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fit\n",
      "   - before_fit     : [TrainEvalCallback, ALUMCallback, Recorder, ProgressCallback]\n",
      "  Start Epoch Loop\n",
      "     - before_epoch   : [Recorder, ProgressCallback]\n",
      "    Start Train\n",
      "       - before_train   : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - before_batch   : [ALUMCallback]\n",
      "         - after_pred     : []\n",
      "         - after_loss     : [ALUMCallback]\n",
      "         - before_backward: []\n",
      "         - before_step    : []\n",
      "         - after_step     : []\n",
      "         - after_cancel_batch: []\n",
      "         - after_batch    : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      End Batch Loop\n",
      "    End Train\n",
      "     - after_cancel_train: [Recorder]\n",
      "     - after_train    : [Recorder, ProgressCallback]\n",
      "    Start Valid\n",
      "       - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - **CBs same as train batch**: []\n",
      "      End Batch Loop\n",
      "    End Valid\n",
      "     - after_cancel_validate: [Recorder]\n",
      "     - after_validate : [Recorder, ProgressCallback]\n",
      "  End Epoch Loop\n",
      "   - after_cancel_epoch: []\n",
      "   - after_epoch    : [Recorder]\n",
      "End Fit\n",
      " - after_cancel_fit: []\n",
      " - after_fit      : [ALUMCallback, ProgressCallback]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.263165</td>\n",
       "      <td>10.850156</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.396017</td>\n",
       "      <td>9.291542</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual adversarial training at epoch 0\n",
      "Your model is probably not supported, make sure model interface is compatible with HF pretrained models\n"
     ]
    }
   ],
   "source": [
    "learn.fit(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMART\n",
    "\n",
    "[SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization](https://arxiv.org/abs/1911.03437)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_ema_model(ema_model:nn.Module, model:nn.Module, mom:float=0.99):\n",
    "    \"Updates `ema_model` parameters with online `model` parameters using momentum `mom`\"\n",
    "    coef = 1-mom\n",
    "    for p_ema, p in zip(ema_model.parameters(), model.parameters()):\n",
    "        p_ema.data.mul_(mom)\n",
    "        p_ema.data.add_(p.data, alpha=coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "**Notation:**  \n",
    "$ g_i(\\tilde{x_i}, \\bar{\\theta_i}) = \\frac{1}{|\\mathcal{B}|}\\sum_{x_i \\in \\mathcal{B}} {\\{\\nabla_x \\ell_s (\\mathcal{f}(x_i; \\bar{\\theta}_s), \\mathcal{f}(\\tilde{x_i}; \\bar{\\theta}_s))} $;  \n",
    "$AdamUpdate_{\\mathcal B}$ - ADAM update  for optimizing;  \n",
    "$\\Pi_{\\mathcal A}$ - prjection to $\\mathcal A$\n",
    "\n",
    "**Input:** $T$: total number of iterations, $\\mathcal X$: the dataset, $\\theta_0$: pre-trained model parameters, $S$: total number of iterations for Bregman proximal point method, $\\sigma^2$: variance for random initialization of perturbation, $T_{\\bar{x}}$number of iterations for updating $\\tilde{x_i}$, $\\eta$: lr for updating $\\tilde{x_i}$, $\\beta$: momentum parameter.\n",
    "\n",
    "01: $\\tilde{\\theta_1} \\leftarrow \\theta_0$  \n",
    "02: **for** $t = 1,...,T$ **do**  \n",
    "03: $\\quad$ $\\bar{\\theta_1} \\leftarrow \\theta_t-1$  \n",
    "04: $\\quad$ **for** $s = 1,...,S$ **do**  \n",
    "05: $\\quad \\quad$ Sample $\\mathcal{B}$ from $\\mathcal X$  \n",
    "06: $\\quad \\quad$ $\\tilde{x_i} \\leftarrow x_i + \\nu_i$ where $\\nu_i ~ \\mathcal{N} (0, \\sigma^2$  \n",
    "07: $\\quad \\quad$ **for** $m = 1,...,T_\\bar{x}$ **do**  \n",
    "08: $\\quad \\quad \\quad$ $\\tilde{g_i} \\leftarrow \\frac{g_i(\\tilde{x_i},\\bar{\\theta_s})}{\\|g_i(\\tilde{x_i},\\bar{\\theta_s})\\|_\\infty} $  \n",
    "09: $\\quad \\quad \\quad$ $\\tilde{x_i} \\leftarrow \\Pi_{\\|\\tilde{x_i}-x\\|_\\infty \\le \\epsilon}(\\tilde{x_i} + \\eta \\tilde{g_i})$  \n",
    "10: $\\quad \\quad$ **end for**  \n",
    "11: $\\quad \\quad$ $\\bar{\\theta}_{s+1} \\leftarrow AdamUpdate_\\mathcal{B} (\\bar{\\theta}_s)$  \n",
    "12: $\\quad$ **end for**  \n",
    "13: $\\quad$ $\\theta_t \\leftarrow \\bar{\\theta}_{S}$  \n",
    "14: $\\quad$ $\\tilde{\\theta}_{t+1} \\leftarrow $  \n",
    "15: **end for**\n",
    "\n",
    "**Output:** $\\theta_T$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SMARTCallback(LossCallback):\n",
    "    \"\"\"\n",
    "    SMART callback for HuggingFace pretrained models.\n",
    "    \n",
    "    Combines smoothness-inducing adversarial training and\n",
    "    momentum accelerated Bregman proximal point optimization.\n",
    "    \"\"\"\n",
    "    run_valid = False\n",
    "    order = GradientAccumulation.order-1\n",
    "    @delegates(compute_adversarial_loss)\n",
    "    def __init__(self, m:nn.Module, alpha:float=1., mu:float=1., start_epoch:int=0, criterion=None,\n",
    "                 mask_special_tokens:bool=False, one_token_type=False, **kwargs):\n",
    "        self.hook = None\n",
    "        self.kwargs = kwargs if kwargs else {}\n",
    "        self._do_vat=True\n",
    "        self.mom = 0.99\n",
    "        self.special_tokens_mask, self.token_type_mask = None, None\n",
    "        store_attr()\n",
    "    \n",
    "    def before_fit(self):\n",
    "        \"Create and freeze EMA model\"\n",
    "        self.ema_model = deepcopy(self.model)\n",
    "        self.ema_model.eval()\n",
    "        self.ema_model.requires_grad_(False)\n",
    "        \n",
    "        if self.criterion is None:\n",
    "            self.criterion = MSELoss() if isinstance(self.loss_func, nn.MSELoss) else SymmetrizedKL\n",
    "        self.adv_loss_func = partial(compute_adversarial_loss, criterion=self.criterion, **self.kwargs)\n",
    "    \n",
    "    def before_batch(self):\n",
    "        if (self.hook is None) and (self.epoch >= self.start_epoch):\n",
    "            self.hook = Hook(self.m, hook_out)\n",
    "            print(f'Starting virtual adversarial training at epoch {self.epoch}')\n",
    "        if (self.mom == 0.99) & (self.pct_train >= 0.1):\n",
    "            self.mom = 0.999\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self.epoch >= self.start_epoch and self._do_vat:\n",
    "            embed, logits = self.hook.stored, self.pred\n",
    "            model = self.model.hf_model if hasattr(self.model, 'hf_model') else self.model\n",
    "            # \"Bregman\" loss\n",
    "            # TODO make sure labels are not in `xb`\n",
    "            with torch.no_grad():\n",
    "                ema_out = self.ema_model(*self.xb)\n",
    "                ema_logits = ema_out.logits if hasattr(ema_out, 'logits') else ema_out\n",
    "            breg_loss = self.criterion(logits, ema_logits)\n",
    "            self.log_loss(breg_loss, 'breg_loss')\n",
    "            self.learn.loss_grad += breg_loss * self.mu\n",
    "            # adversarial loss\n",
    "            try:\n",
    "                adv_loss = self.adv_loss_func(model, embed, logits, self.special_tokens_mask, self.token_type_mask)\n",
    "                self.log_loss(adv_loss, 'adversarial_loss')\n",
    "            except TypeError as e:\n",
    "                print(\"Your model is probably not supported, make sure model interface is compatible with HF pretrained models\")\n",
    "                adv_loss, self._do_vat = 0, False\n",
    "            self.learn.loss_grad += adv_loss * self.alpha\n",
    "    \n",
    "    def after_step(self):\n",
    "        update_ema_model(self.ema_model, self.model, self.mom)\n",
    "    \n",
    "    def after_fit(self):\n",
    "        if self.hook is not None: self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1,10, bias=False),\n",
    "    nn.Linear(10,1, bias=False)\n",
    ")\n",
    "learn = synth_learner(model=model, cbs=SMARTCallback(model[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.511720</td>\n",
       "      <td>9.756591</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.275883</td>\n",
       "      <td>8.746477</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual adversarial training at epoch 0\n",
      "Your model is probably not supported, make sure model interface is compatible with HF pretrained models\n"
     ]
    }
   ],
   "source": [
    "learn.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert learn.smart.mom == 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VATCallback(LossCallback):\n",
    "    \"VAT callback (draft)\"\n",
    "    run_valid=False\n",
    "    # mb worth adding capability to inject adversarial noize into intermediate activations\n",
    "    # for ALUM case we could perturb outputs of the embedding layer instead of embedding weights (which would be equivalent)\n",
    "    def __init__(self, start_iter=None): #?? potentially start in the middle of training\n",
    "        \n",
    "        self.start_iter = start_iter\n",
    "        \n",
    "    def after_loss(self):\n",
    "        #TODO: detach as appropriate\n",
    "        noize = 0\n",
    "        x_adv = self.x + noize #?? take care of possible multiple inputs \n",
    "        logits = self.pred\n",
    "        print(f'{self.train_iter:2} - Do stuff here with input of shape {self.x.shape} and logits {logits.shape} and modify loss {self.loss:.4f}')\n",
    "        # do VAT stuff here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.183575</td>\n",
       "      <td>10.959995</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 16.1694\n",
      " 1 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 8.1717\n",
      " 2 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 4.7342\n",
      " 3 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 14.8299\n",
      " 4 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 15.2079\n",
      " 5 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 9.9517\n",
      " 6 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 13.5607\n",
      " 7 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 6.2763\n",
      " 8 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 12.1665\n",
      " 9 - Do stuff here with input of shape torch.Size([16, 1]) and logits torch.Size([16, 1]) and modify loss 10.9689\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1,10, bias=False),\n",
    "    nn.Linear(10,1, bias=False)\n",
    ")\n",
    "learn = synth_learner(model=model, cbs=VATCallback())\n",
    "learn.fit(1, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
