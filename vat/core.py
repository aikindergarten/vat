# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_core.ipynb (unless otherwise specified).

__all__ = ['hook_out', 'KL', 'adv_project', 'compute_adversarial_loss', 'ALUMCallback', 'VATCallback']

# Cell
from torch import linalg as LA

from fastai.basics import *
from fastai.test_utils import synth_learner
from fastai.callback.all import *

# Cell
def hook_out(m, inp, out):
    return out

# Cell
def KL(input, target, reduction="sum"):
    input = input.float()
    target = target.float()
    loss = F.kl_div(F.log_softmax(input, dim=-1, dtype=torch.float32), F.softmax(target, dim=-1, dtype=torch.float32), reduction=reduction)
    return loss

# Cell
def adv_project(grad, norm_type='inf', eps=1e-6):
    if norm_type == 'l2':
        direction = grad / (torch.norm(grad, dim=-1, keepdim=True) + eps)
    elif norm_type == 'l1':
        direction = grad.sign()
    else:
        direction = grad / (grad.abs().max(-1, keepdim=True)[0] + eps)
    return direction

# Cell
def compute_adversarial_loss(model:nn.Module, embed:Tensor, logits:Tensor,
                             noise_var:float=1e-5, step_size:float=1e-3, k:int=1,
                             noise_gamma:float=1e-6):
    "This is nice docstring"
    noise = embed.data.new(embed.size()).normal_(0, noise_var)
    noise.requires_grad_();

    for _ in range(k):
        newembed = embed + noise
        adv_logits = model(inputs_embeds=newembed).logits

        adv_loss = KL(adv_logits, logits.detach(), reduction="batchmean")
        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True)

        norm = LA.norm(delta_grad)
        if (torch.isnan(norm) or torch.isinf(norm)):
            break

        noise = noise + delta_grad * step_size
        noise = adv_project(noise, norm_type="fro", eps=noise_gamma)

    newembed = embed + noise
    adv_logits = model(inputs_embeds=newembed).logits

    adv_loss_f = KL(adv_logits, logits.detach())
    adv_loss_b = KL(logits, adv_logits.detach())
    return adv_loss_f + adv_loss_b

# Cell
class ALUMCallback(Callback):
    "ALUM callback for HuggingFace pretrained models"
    run_valid = False
    order = GradientAccumulation.order-1
    @delegates(compute_adversarial_loss)
    def __init__(self, m:nn.Module, alpha:float=1., start_epoch:int=1, **kwargs):
        self.hook = None
        self.adv_loss_func = partial(compute_adversarial_loss, **kwargs) if kwargs else compute_adversarial_loss
        self._do_vat=True
        store_attr()

    def before_batch(self):
        if (self.hook is None) and (self.epoch >= self.start_epoch):
            self.hook = Hook(self.m, hook_out)
            print(f'Starting virtual adversarial training at epoch {self.epoch}')

    def after_loss(self):
        if self.epoch >= self.start_epoch and self._do_vat:
            embed, logits = self.hook.stored, self.pred
            model = self.model.hf_model if hasattr(self.model, 'hf_model') else self.model
            try:    adv_loss = self.adv_loss_func(model, embed, logits)
            except TypeError as e:
                print("Your model is probably not supported, make sure model interface is compatible with HF pretrained models")
                adv_loss, self._do_vat = 0, False
            self.learn.loss_grad += adv_loss * self.alpha

    def after_fit(self):
        if self.hook is not None: self.hook.remove()

# Cell
class VATCallback(Callback):
    "VAT callback (draft)"
    run_valid=False
    # mb worth adding capability to inject adversarial noize into intermediate activations
    # for ALUM case we could perturb outputs of the embedding layer instead of embedding weights (which would be equivalent)
    def __init__(self, start_iter=None): #?? potentially start in the middle of training

        self.start_iter = start_iter

    def after_loss(self):
        #TODO: detach as appropriate
        noize = 0
        x_adv = self.x + noize #?? take care of possible multiple inputs
        logits = self.pred
        print(f'{self.train_iter:2} - Do stuff here with input of shape {self.x.shape} and logits {logits.shape} and modify loss {self.loss:.4f}')
        # do VAT stuff here
